{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "\n",
    "import myutils\n",
    "importlib.reload(myutils)\n",
    "import myutils as myutils\n",
    "\n",
    "import dataPrepUtils\n",
    "importlib.reload(dataPrepUtils)\n",
    "import dataPrepUtils as dputils\n",
    "\n",
    "import mypytable\n",
    "importlib.reload(mypytable)\n",
    "from mypytable import MyPyTable \n",
    "\n",
    "import myclassifiers\n",
    "importlib.reload(myclassifiers)\n",
    "from myclassifiers import MyKNeighborsClassifier, MySimpleLinearRegressor, MyNaiveBayesClassifier, MyDecisionTreeClassifier, MyRandomForrestClassifier\n",
    "\n",
    "import myevaluation\n",
    "importlib.reload(myevaluation)\n",
    "import myevaluation as myevaluation"
   ]
  },
  {
   "source": [
    "# Prepare data for fitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[2, 3, 1] 3\n[70000.0, 1609676.1, 3149352.2, 4689028.3, 6228704.4, 7768380.5, 9308056.6, 10847732.7, 12387408.8, 13927084.9, 15466761.0]\n"
     ]
    }
   ],
   "source": [
    "# Load data into a mypytable\n",
    "players_table = MyPyTable()\n",
    "players_table.load_from_file(\"cleaned-data.txt\")\n",
    "\n",
    "# create X_train and y_train\n",
    "# four attributes got: career_PER, career_PTS, career_G, draft_year, win shares\n",
    "# will test using win shares and PER\n",
    "per_col = players_table.get_column(\"career_PER\")\n",
    "pts_col = players_table.get_column(\"career_PTS\")\n",
    "games_col = players_table.get_column(\"career_G\")\n",
    "year_col = players_table.get_column(\"draft_year\")\n",
    "ws_col = players_table.get_column(\"career_WS\")\n",
    "# convert PER, PTS, and G to categorical\n",
    "per_cutoffs = dputils.compute_equal_width_cutoffs(per_col, 5)\n",
    "pts_cutoffs = dputils.compute_equal_width_cutoffs(pts_col, 5)\n",
    "games_cutoffs = dputils.compute_equal_width_cutoffs(games_col, 10)\n",
    "ws_cutoffs = dputils.compute_equal_width_cutoffs(ws_col, 5)\n",
    "categorical_per = myutils.convert_to_categorical(per_col, per_cutoffs)\n",
    "categorical_pts = myutils.convert_to_categorical(pts_col, pts_cutoffs)\n",
    "categorical_games = myutils.convert_to_categorical(games_col, games_cutoffs)\n",
    "categorical_ws = myutils.convert_to_categorical(ws_col, ws_cutoffs)\n",
    "X = []\n",
    "for i in range(len(categorical_per)):\n",
    "    temp = []\n",
    "    # will test using win shares and PER\n",
    "    #temp.append(year_col[i])\n",
    "    temp.append(categorical_pts[i])\n",
    "    #temp.append(categorical_per[i])\n",
    "    temp.append(categorical_games[i])\n",
    "    temp.append(categorical_ws[i])\n",
    "    X.append(temp)\n",
    "salaries = players_table.get_column(\"avg_salary\")\n",
    "salaries_cutoffs = dputils.compute_equal_width_cutoffs(salaries, 10)\n",
    "y = myutils.convert_to_categorical(salaries, salaries_cutoffs)\n",
    "player_names = players_table.get_column(\"name\")\n",
    "index = 271\n",
    "print(X[index], y[index])\n",
    "print(salaries_cutoffs)\n",
    "\n"
   ]
  },
  {
   "source": [
    "# kNN, Naive Bayes, Decision Tree Classifiers and Predictive Accuracies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================\nPredictive Accuracy\n=================================\nStratified 10-Fold Cross Validation\nk Nearest Neighbors: accuracy = 0.12377564979480164, error rate = 0.8762243502051984\nDecision Tree: accuracy = 0.7665526675786593, error rate = 0.23344733242134075\nNaive Bayes: accuracy = 0.340109439124487, error rate = 0.659890560875513\n\n"
     ]
    }
   ],
   "source": [
    "# perform stratified k-fold cross validation (k=10)\n",
    "\n",
    "X_train_folds, X_test_folds = myevaluation.stratified_kfold_cross_validation(X, y, 10)\n",
    "X_test = []\n",
    "X_train = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "for fold in X_test_folds:\n",
    "    x_temp = []\n",
    "    y_temp = []\n",
    "    for i in range(len(fold)):\n",
    "        x_temp.append(X[fold[i]].copy())\n",
    "        y_temp.append(y[fold[i]])\n",
    "    X_test.append(x_temp)\n",
    "    y_test.append(y_temp)\n",
    "for fold in X_train_folds:\n",
    "    x_temp = []\n",
    "    y_temp = []\n",
    "    for i in range(len(fold)):\n",
    "        x_temp.append(X[fold[i]].copy())\n",
    "        y_temp.append(y[fold[i]])\n",
    "    X_train.append(x_temp)\n",
    "    y_train.append(y_temp)\n",
    "#print(X_test)\n",
    "# declare classifiers\n",
    "knn_classifier = MyKNeighborsClassifier()\n",
    "n_bayes_classifier = MyNaiveBayesClassifier()\n",
    "d_tree_classifier = MyDecisionTreeClassifier()\n",
    "\n",
    "# test/train with each fold\n",
    "knn_accuracies = []\n",
    "dtree_accuracies = []\n",
    "nbayes_accuracies = []\n",
    "knn_predicted_total = []\n",
    "dtree_predicted_total = []\n",
    "nbayes_predicted_total = []\n",
    "\n",
    "for k in range(len(X_test)):\n",
    "    # fit classifiers using X_train and y_train\n",
    "    knn_classifier.fit(X_train[k], y_train[k])\n",
    "    d_tree_classifier.fit(X_train[k], y_train[k])\n",
    "    X_train_copy = X_train[k].copy() # n_bayes needs a copy since it   modifies X_train\n",
    "    n_bayes_classifier.fit(X_train_copy, y_train[k])\n",
    "    #d_tree_classifier.print_decision_rules(attribute_names=[\"draft year\", \"career pts avg\", \"career per\", \"career games\"],  class_name=\"salary ranking\")\n",
    "    # make predictions\n",
    "    knn_predicted = knn_classifier.predict(X_test[k])\n",
    "    knn_predicted_total.append(knn_predicted)\n",
    "    dtree_predicted = d_tree_classifier.predict(X_test[k])\n",
    "    dtree_predicted_total.append(dtree_predicted)\n",
    "    nbayes_predicted = n_bayes_classifier.predict(X_test[k])\n",
    "    nbayes_predicted_total.append(nbayes_predicted)\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    for i in range(len(knn_predicted)):\n",
    "        if knn_predicted[i] == y_test[k][i]:\n",
    "            count1 += 1\n",
    "        if dtree_predicted[i] == y_test[k][i]:\n",
    "            count2 += 1\n",
    "        if nbayes_predicted[i] == y_test[k][i]:\n",
    "            count3 += 1\n",
    "    knn_accuracies.append(count1 / len(knn_predicted))\n",
    "    dtree_accuracies.append(count2 / len(dtree_predicted))\n",
    "    nbayes_accuracies.append(count3 / len(nbayes_predicted))\n",
    "knn_accuracy = sum(knn_accuracies) / len(knn_accuracies)\n",
    "dtree_accuracy = sum(dtree_accuracies) / len(dtree_accuracies)\n",
    "nbayes_accuracy = sum(nbayes_accuracies) / len(nbayes_accuracies)\n",
    "# flatten total predicted lists\n",
    "knn_predicted_total = [item for sublist in knn_predicted_total for item in sublist]\n",
    "dtree_predicted_total = [item for sublist in dtree_predicted_total for item in sublist]\n",
    "nbayes_predicted_total = [item for sublist in nbayes_predicted_total for item in sublist]\n",
    "# print accuracies\n",
    "print(\"=================================\")\n",
    "print(\"Predictive Accuracy\")\n",
    "print(\"=================================\")\n",
    "print(\"Stratified 10-Fold Cross Validation\")\n",
    "print(\"k Nearest Neighbors: accuracy = \" + str(knn_accuracy) + \", error rate = \" + str(1 - knn_accuracy))\n",
    "print(\"Decision Tree: accuracy = \" + str(dtree_accuracy) + \", error rate = \" + str(1 - dtree_accuracy))\n",
    "print(\"Naive Bayes: accuracy = \" + str(nbayes_accuracy) + \", error rate = \" + str(1 - nbayes_accuracy))\n",
    "print()"
   ]
  },
  {
   "source": [
    "# Confusion Matrices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================\nConfusion Matrices\n=================================\nk-Nearest Neighbors (Stratified 10-Fold Cross Validation Results)\n      1    2    3    4    5    6    7    8    9    10    total    Recognition %\n--  ---  ---  ---  ---  ---  ---  ---  ---  ---  ----  -------  ---------------\n 1    9  126   44   53   22   46   11    5    0     0      316          2.8481\n 2    1   42   13   26   14   14    5    1    0     0      116         36.2069\n 3    5   66   23   21    6   21    9    3    0     0      154         14.9351\n 4    0   29   10   16    7   12    1    3    0     0       78         20.5128\n 5    2   42   15   24    9   18    5    1    0     0      116          7.75862\n 6    4    6    5    5    5    6    2    3    0     0       36         16.6667\n 7    1    5    9    3    1    5    1    1    0     0       26          3.84615\n 8    2    2    1    1    1    1    1    0    0     0        9          0\n 9    0    0    0    0    0    0    0    0    0     0        0          0\n10    1    0    1    0    0    1    2    0    0     0        5          0\n\nDecision Tree (Stratified 10-Fold Cross Validation Results)\n      1    2    3    4    5    6    7    8    9    10    total    Recognition %\n--  ---  ---  ---  ---  ---  ---  ---  ---  ---  ----  -------  ---------------\n 1  275   40    1    0    0    0    0    0    0     0      316          87.0253\n 2    1  109    0    5    1    0    0    0    0     0      116          93.9655\n 3   57    0   88    8    1    0    0    0    0     0      154          57.1429\n 4    1    0    3   64    7    0    3    0    0     0       78          82.0513\n 5    4    0    9    4   96    1    2    0    0     0      116          82.7586\n 6    2    9    0   10    3   12    0    0    0     0       36          33.3333\n 7    0    4    2   12    0    0    7    0    0     1       26          26.9231\n 8    0    0    0    1    0    2    3    3    0     0        9          33.3333\n 9    0    0    0    0    0    0    0    0    0     0        0           0\n10    0    0    0    0    0    1    2    0    0     2        5          40\n\nNaive Bayes (Stratified 10-Fold Cross Validation Results)\n      1    2    3    4    5    6    7    8    9    10    total    Recognition %\n--  ---  ---  ---  ---  ---  ---  ---  ---  ---  ----  -------  ---------------\n 1  200    0   89    1    6    0   20    0    0     0      316         63.2911\n 2   18    0    0   18   79    0    0    1    0     0      116          0\n 3  103    0    3   31    1    0    1   15    0     0      154          1.94805\n 4   47   18    8    0    2    0    3    0    0     0       78          0\n 5    4   18    0   28   65    1    0    0    0     0      116         56.0345\n 6    0    0    3    1    7   13    9    3    0     0       36         36.1111\n 7    4    0    2    0    7    7    6    0    0     0       26         23.0769\n 8    0    0    2    0    0    0    3    4    0     0        9         44.4444\n 9    0    0    0    0    0    0    0    0    0     0        0          0\n10    0    0    2    0    0    1    2    0    0     0        5          0\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrices\n",
    "categories = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# flatten y_test\n",
    "y_test_total = [item for sublist in y_test for item in sublist]\n",
    "knn_matrix = myevaluation.confusion_matrix(y_test_total, knn_predicted_total,  categories)\n",
    "d_tree_matrix = myevaluation.confusion_matrix(y_test_total, dtree_predicted_total, categories)\n",
    "n_bayes_matrix = myevaluation.confusion_matrix(y_test_total, nbayes_predicted_total, categories)\n",
    "\n",
    "print(\"=================================\")\n",
    "print(\"Confusion Matrices\")\n",
    "print(\"=================================\")\n",
    "\n",
    "# create matrix header\n",
    "header = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"total\", \"Recognition %\"]\n",
    "\n",
    "# knn matrix\n",
    "for i in range(len(knn_matrix)): \n",
    "    total = sum(knn_matrix[i])\n",
    "    knn_matrix[i].append(total)\n",
    "    recognition = 0\n",
    "    if total != 0: \n",
    "        recognition = ((knn_matrix[i][i]/total) * 100)\n",
    "    knn_matrix[i].append(recognition)\n",
    "    knn_matrix[i].insert(0, (i + 1))\n",
    "print(\"k-Nearest Neighbors (Stratified 10-Fold Cross Validation Results)\")\n",
    "print(tabulate(knn_matrix, headers= header))\n",
    "print()\n",
    "\n",
    "# decision tree matrix\n",
    "for i in range(len(d_tree_matrix)): \n",
    "    total = sum(d_tree_matrix[i])\n",
    "    d_tree_matrix[i].append(total)\n",
    "    recognition = 0\n",
    "    if total != 0: \n",
    "        recognition = ((d_tree_matrix[i][i]/total) * 100)\n",
    "    d_tree_matrix[i].append(recognition)\n",
    "    d_tree_matrix[i].insert(0, (i + 1))\n",
    "print(\"Decision Tree (Stratified 10-Fold Cross Validation Results)\")\n",
    "print(tabulate(d_tree_matrix, headers= header))\n",
    "print()\n",
    "\n",
    "# naive bayes matrix\n",
    "for i in range(len(n_bayes_matrix)): \n",
    "    total = sum(n_bayes_matrix[i])\n",
    "    n_bayes_matrix[i].append(total)\n",
    "    recognition = 0\n",
    "    if total != 0: \n",
    "        recognition = ((n_bayes_matrix[i][i]/total) * 100)\n",
    "    n_bayes_matrix[i].append(recognition)\n",
    "    n_bayes_matrix[i].insert(0, (i + 1))\n",
    "print(\"Naive Bayes (Stratified 10-Fold Cross Validation Results)\")\n",
    "print(tabulate(n_bayes_matrix, headers= header))"
   ]
  },
  {
   "source": [
    "# Test Random Forest Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Prepare Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare all 10 attributes for selection\n",
    "per_col = players_table.get_column(\"career_PER\")\n",
    "pts_col = players_table.get_column(\"career_PTS\")\n",
    "games_col = players_table.get_column(\"career_G\")\n",
    "ws_col = players_table.get_column(\"career_WS\")\n",
    "ast_col = players_table.get_column(\"career_AST\")\n",
    "fg_col = players_table.get_column(\"career_FG%\")\n",
    "fg3_col = players_table.get_column(\"career_FG3%\")\n",
    "ft_col = players_table.get_column(\"career_FT%\")\n",
    "trb_col = players_table.get_column(\"career_TRB\")\n",
    "efg_col = players_table.get_column(\"career_eFG%\")\n",
    "# convert attributes to categorical values\n",
    "per_cutoffs = dputils.compute_equal_width_cutoffs(per_col, 5)\n",
    "pts_cutoffs = dputils.compute_equal_width_cutoffs(pts_col, 5)\n",
    "games_cutoffs = dputils.compute_equal_width_cutoffs(games_col, 10)\n",
    "ws_cutoffs = dputils.compute_equal_width_cutoffs(ws_col, 5)\n",
    "categorical_per = myutils.convert_to_categorical(per_col, per_cutoffs)\n",
    "categorical_pts = myutils.convert_to_categorical(pts_col, pts_cutoffs)\n",
    "categorical_games = myutils.convert_to_categorical(games_col, games_cutoffs)\n",
    "categorical_ws = myutils.convert_to_categorical(ws_col, ws_cutoffs)\n",
    "ast_cutoffs = dputils.compute_equal_width_cutoffs(ast_col, 5)\n",
    "fg_cutoffs = dputils.compute_equal_width_cutoffs(fg_col, 10)\n",
    "fg3_cutoffs = dputils.compute_equal_width_cutoffs(fg3_col, 10)\n",
    "players_table.remove_rows_with_missing_values()\n",
    "ft_cutoffs = dputils.compute_equal_width_cutoffs(ft_col, 5)\n",
    "trb_cutoffs = dputils.compute_equal_width_cutoffs(trb_col, 5)\n",
    "efg_cutoffs = dputils.compute_equal_width_cutoffs(efg_col, 10)\n",
    "categorical_ast = myutils.convert_to_categorical(ast_col, ast_cutoffs)\n",
    "categorical_fg = myutils.convert_to_categorical(fg_col, fg_cutoffs)\n",
    "categorical_fg3 = myutils.convert_to_categorical(fg3_col, fg3_cutoffs)\n",
    "categorical_ft = myutils.convert_to_categorical(ft_col, ft_cutoffs)\n",
    "categorical_trb = myutils.convert_to_categorical(trb_col, trb_cutoffs)\n",
    "categorical_efg = myutils.convert_to_categorical(efg_col, efg_cutoffs)"
   ]
  },
  {
   "source": [
    "## Prepare Train Sets From Columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(len(categorical_per)):\n",
    "    temp = []\n",
    "    temp.append(categorical_pts[i])\n",
    "    temp.append(categorical_per[i])\n",
    "    temp.append(categorical_games[i])\n",
    "    temp.append(categorical_ws[i])\n",
    "    temp.append(categorical_ast[i])\n",
    "    temp.append(categorical_fg[i])\n",
    "    temp.append(categorical_fg3[i])\n",
    "    temp.append(categorical_ft[i])\n",
    "    temp.append(categorical_trb[i])\n",
    "    temp.append(categorical_efg[i])\n",
    "    X.append(temp)\n"
   ]
  },
  {
   "source": [
    "# Fit and Predict Using Random Forrest Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================\nPredictive Accuracy: 0.06658878504672897\n=================================\n=================================\nError Rate: 0.9334112149532711\n=================================\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(myutils)\n",
    "rf_classifier = MyRandomForrestClassifier()\n",
    "rf_classifier.fit(X, y)\n",
    "X_test = []\n",
    "y_test = []\n",
    "for test in rf_classifier.test_set:\n",
    "    X_test.append(test[0])\n",
    "    y_test.append(test[1])\n",
    "predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "#Calculate accuracy \n",
    "correct = 0 \n",
    "total = len(X)\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == y_test[i]:\n",
    "        correct += 1\n",
    "accuracy = correct/total\n",
    "error = 1 - accuracy\n",
    "print(\"=================================\")\n",
    "print(\"Predictive Accuracy:\", accuracy)\n",
    "print(\"=================================\")\n",
    "print(\"=================================\")\n",
    "print(\"Error Rate:\", error)\n",
    "print(\"=================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}