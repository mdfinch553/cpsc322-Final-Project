{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python385jvsc74a57bd098b0a9b7b4eaaa670588a142fd0a9b87eaafe866f1db4228be72b4211d12040f",
   "display_name": "Python 3.8.5 64-bit ('base': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "from tabulate import tabulate\n",
    "import os\n",
    "\n",
    "import mysklearn.myutils\n",
    "importlib.reload(mysklearn.myutils)\n",
    "import mysklearn.myutils as myutils\n",
    "\n",
    "import mysklearn.dataPrepUtils\n",
    "importlib.reload(mysklearn.dataPrepUtils)\n",
    "import mysklearn.dataPrepUtils as dputils\n",
    "\n",
    "import mysklearn.mypytable\n",
    "importlib.reload(mysklearn.mypytable)\n",
    "from mysklearn.mypytable import MyPyTable \n",
    "\n",
    "import mysklearn.myclassifiers\n",
    "importlib.reload(mysklearn.myclassifiers)\n",
    "from mysklearn.myclassifiers import MyKNeighborsClassifier, MySimpleLinearRegressor, MyNaiveBayesClassifier, MyDecisionTreeClassifier, MyRandomForrestClassifier\n",
    "\n",
    "import mysklearn.myevaluation\n",
    "importlib.reload(mysklearn.myevaluation)\n",
    "import mysklearn.myevaluation as myevaluation"
   ]
  },
  {
   "source": [
    "# Prepare data for fitting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[3, 3, 1] 3\n[70000.0, 1609676.1, 3149352.2, 4689028.3, 6228704.4, 7768380.5, 9308056.6, 10847732.7, 12387408.8, 13927084.9, 15466761.0]\n"
     ]
    }
   ],
   "source": [
    "# Load data into a mypytable\n",
    "players_table = MyPyTable()\n",
    "players_table.load_from_file(\"input-data/cleaned-data.txt\")\n",
    "\n",
    "# create X_train and y_train\n",
    "# four attributes got: career_PER, career_PTS, career_G, draft_year, win shares\n",
    "# will test using win shares and PER\n",
    "per_col = players_table.get_column(\"career_PER\")\n",
    "pts_col = players_table.get_column(\"career_PTS\")\n",
    "games_col = players_table.get_column(\"career_G\")\n",
    "year_col = players_table.get_column(\"draft_year\")\n",
    "ws_col = players_table.get_column(\"career_WS\")\n",
    "# convert PER, PTS, and G to categorical\n",
    "per_cutoffs = dputils.compute_equal_width_cutoffs(per_col, 5)\n",
    "pts_cutoffs = dputils.compute_equal_width_cutoffs(pts_col, 5)\n",
    "games_cutoffs = dputils.compute_equal_width_cutoffs(games_col, 10)\n",
    "ws_cutoffs = dputils.compute_equal_width_cutoffs(ws_col, 5)\n",
    "categorical_per = myutils.convert_to_categorical(per_col, per_cutoffs)\n",
    "categorical_pts = myutils.convert_to_categorical(pts_col, pts_cutoffs)\n",
    "categorical_games = myutils.convert_to_categorical(games_col, games_cutoffs)\n",
    "categorical_ws = myutils.convert_to_categorical(ws_col, ws_cutoffs)\n",
    "X = []\n",
    "for i in range(len(categorical_per)):\n",
    "    temp = []\n",
    "    # will test using win shares and PER\n",
    "    #temp.append(year_col[i])\n",
    "    #temp.append(categorical_pts[i])\n",
    "    temp.append(categorical_per[i])\n",
    "    temp.append(categorical_games[i])\n",
    "    temp.append(categorical_ws[i])\n",
    "    X.append(temp)\n",
    "salaries = players_table.get_column(\"avg_salary\")\n",
    "salaries_cutoffs = dputils.compute_equal_width_cutoffs(salaries, 10)\n",
    "y = myutils.convert_to_categorical(salaries, salaries_cutoffs)\n",
    "player_names = players_table.get_column(\"name\")\n",
    "index = 271\n",
    "print(X[index], y[index])\n",
    "print(salaries_cutoffs)\n",
    "\n"
   ]
  },
  {
   "source": [
    "# kNN, Naive Bayes, Decision Tree Classifiers and Predictive Accuracies"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================\nPredictive Accuracy\n=================================\nStratified 10-Fold Cross Validation\nk Nearest Neighbors: accuracy = 0.22554035567715455, error rate = 0.7744596443228454\nDecision Tree: accuracy = 0.8555950752393983, error rate = 0.14440492476060174\nNaive Bayes: accuracy = 0.3386593707250342, error rate = 0.6613406292749657\n\n"
     ]
    }
   ],
   "source": [
    "# perform stratified k-fold cross validation (k=10)\n",
    "\n",
    "X_train_folds, X_test_folds = myevaluation.stratified_kfold_cross_validation(X, y, 10)\n",
    "X_test = []\n",
    "X_train = []\n",
    "y_train = []\n",
    "y_test = []\n",
    "for fold in X_test_folds:\n",
    "    x_temp = []\n",
    "    y_temp = []\n",
    "    for i in range(len(fold)):\n",
    "        x_temp.append(X[fold[i]].copy())\n",
    "        y_temp.append(y[fold[i]])\n",
    "    X_test.append(x_temp)\n",
    "    y_test.append(y_temp)\n",
    "for fold in X_train_folds:\n",
    "    x_temp = []\n",
    "    y_temp = []\n",
    "    for i in range(len(fold)):\n",
    "        x_temp.append(X[fold[i]].copy())\n",
    "        y_temp.append(y[fold[i]])\n",
    "    X_train.append(x_temp)\n",
    "    y_train.append(y_temp)\n",
    "#print(X_test)\n",
    "# declare classifiers\n",
    "knn_classifier = MyKNeighborsClassifier()\n",
    "n_bayes_classifier = MyNaiveBayesClassifier()\n",
    "d_tree_classifier = MyDecisionTreeClassifier()\n",
    "\n",
    "# test/train with each fold\n",
    "knn_accuracies = []\n",
    "dtree_accuracies = []\n",
    "nbayes_accuracies = []\n",
    "knn_predicted_total = []\n",
    "dtree_predicted_total = []\n",
    "nbayes_predicted_total = []\n",
    "\n",
    "for k in range(len(X_test)):\n",
    "    # fit classifiers using X_train and y_train\n",
    "    knn_classifier.fit(X_train[k], y_train[k])\n",
    "    d_tree_classifier.fit(X_train[k], y_train[k])\n",
    "    X_train_copy = X_train[k].copy() # n_bayes needs a copy since it   modifies X_train\n",
    "    n_bayes_classifier.fit(X_train_copy, y_train[k])\n",
    "    #d_tree_classifier.print_decision_rules(attribute_names=[\"draft year\", \"career pts avg\", \"career per\", \"career games\"],  class_name=\"salary ranking\")\n",
    "    # make predictions\n",
    "    knn_predicted = knn_classifier.predict(X_test[k])\n",
    "    knn_predicted_total.append(knn_predicted)\n",
    "    dtree_predicted = d_tree_classifier.predict(X_test[k])\n",
    "    dtree_predicted_total.append(dtree_predicted)\n",
    "    nbayes_predicted = n_bayes_classifier.predict(X_test[k])\n",
    "    nbayes_predicted_total.append(nbayes_predicted)\n",
    "    count1 = 0\n",
    "    count2 = 0\n",
    "    count3 = 0\n",
    "    for i in range(len(knn_predicted)):\n",
    "        if knn_predicted[i] == y_test[k][i]:\n",
    "            count1 += 1\n",
    "        if dtree_predicted[i] == y_test[k][i]:\n",
    "            count2 += 1\n",
    "        if nbayes_predicted[i] == y_test[k][i]:\n",
    "            count3 += 1\n",
    "    knn_accuracies.append(count1 / len(knn_predicted))\n",
    "    dtree_accuracies.append(count2 / len(dtree_predicted))\n",
    "    nbayes_accuracies.append(count3 / len(nbayes_predicted))\n",
    "knn_accuracy = sum(knn_accuracies) / len(knn_accuracies)\n",
    "dtree_accuracy = sum(dtree_accuracies) / len(dtree_accuracies)\n",
    "nbayes_accuracy = sum(nbayes_accuracies) / len(nbayes_accuracies)\n",
    "# flatten total predicted lists\n",
    "knn_predicted_total = [item for sublist in knn_predicted_total for item in sublist]\n",
    "dtree_predicted_total = [item for sublist in dtree_predicted_total for item in sublist]\n",
    "nbayes_predicted_total = [item for sublist in nbayes_predicted_total for item in sublist]\n",
    "# print accuracies\n",
    "print(\"=================================\")\n",
    "print(\"Predictive Accuracy\")\n",
    "print(\"=================================\")\n",
    "print(\"Stratified 10-Fold Cross Validation\")\n",
    "print(\"k Nearest Neighbors: accuracy = \" + str(knn_accuracy) + \", error rate = \" + str(1 - knn_accuracy))\n",
    "print(\"Decision Tree: accuracy = \" + str(dtree_accuracy) + \", error rate = \" + str(1 - dtree_accuracy))\n",
    "print(\"Naive Bayes: accuracy = \" + str(nbayes_accuracy) + \", error rate = \" + str(1 - nbayes_accuracy))\n",
    "print()"
   ]
  },
  {
   "source": [
    "# Confusion Matrices"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================\nConfusion Matrices\n=================================\nk-Nearest Neighbors (Stratified 10-Fold Cross Validation Results)\n      1    2    3    4    5    6    7    8    9    10    total    Recognition %\n--  ---  ---  ---  ---  ---  ---  ---  ---  ---  ----  -------  ---------------\n 1   94  247    4   14   37   20   26    1    0    12      455         20.6593\n 2   28   81    2    5   11    9    6    1    0     6      149         54.3624\n 3    6   15    2    3    2    7    0    0    0     1       36          5.55556\n 4   13   52    2    6   15    3    0    0    0     3       94          6.38298\n 5    7   27    3    2    7    3    1    0    0     1       51         13.7255\n 6    9   15    0    2    4    2    3    1    0     2       38          5.26316\n 7    2    2    2    1    1    0    0    0    0     1        9          0\n 8    4    1    0    0    4    0    3    0    0     2       14          0\n 9    0    0    0    0    0    0    0    0    0     0        0          0\n10    4    0    2    0    0    1    1    1    0     1       10         10\n\nDecision Tree (Stratified 10-Fold Cross Validation Results)\n      1    2    3    4    5    6    7    8    9    10    total    Recognition %\n--  ---  ---  ---  ---  ---  ---  ---  ---  ---  ----  -------  ---------------\n 1  416   24    0   11    2    2    0    0    0     0      455          91.4286\n 2    0  141    0    4    3    1    0    0    0     0      149          94.6309\n 3    0    8   19    7    0    1    0    0    0     1       36          52.7778\n 4    0    6    0   76    5    3    0    4    0     0       94          80.8511\n 5    3    3    0    1   41    1    0    2    0     0       51          80.3922\n 6    0    0    0    0    0   38    0    0    0     0       38         100\n 7    0    6    0    0    0    1    1    1    0     0        9          11.1111\n 8    3    0    0    5    1    4    1    0    0     0       14           0\n 9    0    0    0    0    0    0    0    0    0     0        0           0\n10    0    5    2    0    0    2    0    1    0     0       10           0\n\nNaive Bayes (Stratified 10-Fold Cross Validation Results)\n      1    2    3    4    5    6    7    8    9    10    total    Recognition %\n--  ---  ---  ---  ---  ---  ---  ---  ---  ---  ----  -------  ---------------\n 1  196    0   19  165   63    1    0   11    0     0      455         43.0769\n 2   68    2    7   31   41    0    0    0    0     0      149          1.34228\n 3    0    1    3    0   12    0    0   18    0     2       36          8.33333\n 4   12   20    2   52    4    4    0    0    0     0       94         55.3191\n 5    3   17    0    1   27    0    1    2    0     0       51         52.9412\n 6    0    0    0   35    3    0    0    0    0     0       38          0\n 7    3    0    2    3    0    0    0    1    0     0        9          0\n 8    1    0    0    0    1    0    1   10    0     1       14         71.4286\n 9    0    0    0    0    0    0    0    0    0     0        0          0\n10    0    0    0    0    0    2    0    8    0     0       10          0\n"
     ]
    }
   ],
   "source": [
    "# Confusion Matrices\n",
    "categories = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "# flatten y_test\n",
    "y_test_total = [item for sublist in y_test for item in sublist]\n",
    "knn_matrix = myevaluation.confusion_matrix(y_test_total, knn_predicted_total,  categories)\n",
    "d_tree_matrix = myevaluation.confusion_matrix(y_test_total, dtree_predicted_total, categories)\n",
    "n_bayes_matrix = myevaluation.confusion_matrix(y_test_total, nbayes_predicted_total, categories)\n",
    "\n",
    "print(\"=================================\")\n",
    "print(\"Confusion Matrices\")\n",
    "print(\"=================================\")\n",
    "\n",
    "# create matrix header\n",
    "header = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\", \"total\", \"Recognition %\"]\n",
    "\n",
    "# knn matrix\n",
    "for i in range(len(knn_matrix)): \n",
    "    total = sum(knn_matrix[i])\n",
    "    knn_matrix[i].append(total)\n",
    "    recognition = 0\n",
    "    if total != 0: \n",
    "        recognition = ((knn_matrix[i][i]/total) * 100)\n",
    "    knn_matrix[i].append(recognition)\n",
    "    knn_matrix[i].insert(0, (i + 1))\n",
    "print(\"k-Nearest Neighbors (Stratified 10-Fold Cross Validation Results)\")\n",
    "print(tabulate(knn_matrix, headers= header))\n",
    "print()\n",
    "\n",
    "# decision tree matrix\n",
    "for i in range(len(d_tree_matrix)): \n",
    "    total = sum(d_tree_matrix[i])\n",
    "    d_tree_matrix[i].append(total)\n",
    "    recognition = 0\n",
    "    if total != 0: \n",
    "        recognition = ((d_tree_matrix[i][i]/total) * 100)\n",
    "    d_tree_matrix[i].append(recognition)\n",
    "    d_tree_matrix[i].insert(0, (i + 1))\n",
    "print(\"Decision Tree (Stratified 10-Fold Cross Validation Results)\")\n",
    "print(tabulate(d_tree_matrix, headers= header))\n",
    "print()\n",
    "\n",
    "# naive bayes matrix\n",
    "for i in range(len(n_bayes_matrix)): \n",
    "    total = sum(n_bayes_matrix[i])\n",
    "    n_bayes_matrix[i].append(total)\n",
    "    recognition = 0\n",
    "    if total != 0: \n",
    "        recognition = ((n_bayes_matrix[i][i]/total) * 100)\n",
    "    n_bayes_matrix[i].append(recognition)\n",
    "    n_bayes_matrix[i].insert(0, (i + 1))\n",
    "print(\"Naive Bayes (Stratified 10-Fold Cross Validation Results)\")\n",
    "print(tabulate(n_bayes_matrix, headers= header))"
   ]
  },
  {
   "source": [
    "# Test Random Forest Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Prepare Data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare all 10 attributes for selection\n",
    "per_col = players_table.get_column(\"career_PER\")\n",
    "pts_col = players_table.get_column(\"career_PTS\")\n",
    "games_col = players_table.get_column(\"career_G\")\n",
    "ws_col = players_table.get_column(\"career_WS\")\n",
    "ast_col = players_table.get_column(\"career_AST\")\n",
    "fg_col = players_table.get_column(\"career_FG%\")\n",
    "fg3_col = players_table.get_column(\"career_FG3%\")\n",
    "ft_col = players_table.get_column(\"career_FT%\")\n",
    "trb_col = players_table.get_column(\"career_TRB\")\n",
    "efg_col = players_table.get_column(\"career_eFG%\")\n",
    "# convert attributes to categorical values\n",
    "per_cutoffs = dputils.compute_equal_width_cutoffs(per_col, 5)\n",
    "pts_cutoffs = dputils.compute_equal_width_cutoffs(pts_col, 5)\n",
    "games_cutoffs = dputils.compute_equal_width_cutoffs(games_col, 10)\n",
    "ws_cutoffs = dputils.compute_equal_width_cutoffs(ws_col, 5)\n",
    "categorical_per = myutils.convert_to_categorical(per_col, per_cutoffs)\n",
    "categorical_pts = myutils.convert_to_categorical(pts_col, pts_cutoffs)\n",
    "categorical_games = myutils.convert_to_categorical(games_col, games_cutoffs)\n",
    "categorical_ws = myutils.convert_to_categorical(ws_col, ws_cutoffs)\n",
    "ast_cutoffs = dputils.compute_equal_width_cutoffs(ast_col, 5)\n",
    "fg_cutoffs = dputils.compute_equal_width_cutoffs(fg_col, 10)\n",
    "fg3_cutoffs = dputils.compute_equal_width_cutoffs(fg3_col, 10)\n",
    "players_table.remove_rows_with_missing_values()\n",
    "ft_cutoffs = dputils.compute_equal_width_cutoffs(ft_col, 5)\n",
    "trb_cutoffs = dputils.compute_equal_width_cutoffs(trb_col, 5)\n",
    "efg_cutoffs = dputils.compute_equal_width_cutoffs(efg_col, 10)\n",
    "categorical_ast = myutils.convert_to_categorical(ast_col, ast_cutoffs)\n",
    "categorical_fg = myutils.convert_to_categorical(fg_col, fg_cutoffs)\n",
    "categorical_fg3 = myutils.convert_to_categorical(fg3_col, fg3_cutoffs)\n",
    "categorical_ft = myutils.convert_to_categorical(ft_col, ft_cutoffs)\n",
    "categorical_trb = myutils.convert_to_categorical(trb_col, trb_cutoffs)\n",
    "categorical_efg = myutils.convert_to_categorical(efg_col, efg_cutoffs)\n",
    "\n",
    "#get salaries\n",
    "salaries = players_table.get_column(\"avg_salary\")\n",
    "salaries_cutoffs = dputils.compute_equal_width_cutoffs(salaries, 10)\n",
    "y_train = myutils.convert_to_categorical(salaries, salaries_cutoffs)"
   ]
  },
  {
   "source": [
    "## Prepare Train Sets From Columns"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = []\n",
    "for i in range(len(categorical_per)):\n",
    "    temp = []\n",
    "    temp.append(categorical_pts[i])\n",
    "    temp.append(categorical_per[i])\n",
    "    temp.append(categorical_games[i])\n",
    "    temp.append(categorical_ws[i])\n",
    "    #temp.append(categorical_ast[i])\n",
    "    #temp.append(categorical_fg[i])\n",
    "    #temp.append(categorical_fg3[i])\n",
    "    #temp.append(categorical_ft[i])\n",
    "    #temp.append(categorical_trb[i])\n",
    "    #temp.append(categorical_efg[i])\n",
    "    X.append(temp)\n"
   ]
  },
  {
   "source": [
    "# Fit and Predict Using Random Forest Classifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "=================================\nPredictive Accuracy: 0.11458333333333333\n=================================\n=================================\nError Rate: 0.8854166666666666\n=================================\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(myutils)\n",
    "rf_classifier = MyRandomForrestClassifier(3,10)\n",
    "rf_classifier.fit(X, y_train)\n",
    "X_test = []\n",
    "y_test = []\n",
    "for test in rf_classifier.test_set:\n",
    "    X_test.append(test[0])\n",
    "    y_test.append(test[1])\n",
    "predictions = rf_classifier.predict(X_test)\n",
    "\n",
    "#Calculate accuracy \n",
    "correct = 0 \n",
    "total = len(predictions)\n",
    "for i in range(len(predictions)):\n",
    "    if predictions[i] == y_test[i]:\n",
    "        correct += 1\n",
    "accuracy = correct/total\n",
    "error = 1 - accuracy\n",
    "print(\"=================================\")\n",
    "print(\"Predictive Accuracy:\", accuracy)\n",
    "print(\"=================================\")\n",
    "print(\"=================================\")\n",
    "print(\"Error Rate:\", error)\n",
    "print(\"=================================\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}